---
layout: chapter
title: "Pack 4: Responsiveness in Adaptation"
lang: en-gb
alt_lang_url: "/tw/4"
permalink: "/4/"
---

A clinic posts hours; patients show up to a locked door. A responsive clinic apologises, posts why it happened, updates hours, and texts people next time. The fix becomes part of the system.

Competent action creates new information. Refusing to hear it is the fastest path to failure. Tronto defines responsiveness as "observing that response and making judgements about it — whether the care given was sufficient, successful, or complete." Crucially, "the person cared for need not be the one who responds" — others in the care setting can assess. And "new needs will undoubtedly arise," restarting the cycle back to attentiveness.

The illustration's framing: _we check the results_ — not metrics that don't measure what we value, but metrics designed by and for the people affected.

### Core ideas

- **People closest to harm define harm.** They get the author pen for evaluations.
- **Right to reply is a right to improvement.** A reply that cannot cause change is theatre.
- **Shared memory.** Post-incident learnings become tests; tests prevent repeats.
- **Time as a service.** A fast wrong-then-right beats a slow maybe. Use reversible defaults.

### What good responsiveness looks like

- **Community-designed reward.** Train agents with Reinforcement Learning from Community Feedback (RLCF): optimise for cross-group endorsement and trust-under-loss, not raw engagement. The community defines what "good" means — and the definition evolves.
- **Weval-style registries.** A "Wikipedia for evals": anyone can draft an eval; civil society partners peer-review; labs adopt or explain. These are not lab-designed benchmarks vendors can teach to the test — they are living, community-maintained test suites.
- **Clear appeals.** Urgent cases answered in 48 hours, standard in 7 days, complex in 30 days. Remedies include correction, rollback, or compensation.
- **Incident run-books.** S0–S3 severity with playbooks, on-call roles, and communication templates.
- **Public repair log.** Each incident has a page: what happened, whom it hit, fixes, dates, and the test that now guards against it.

### From ideas to practice

1. **Expose the appeal button.** Everywhere a decision is shown, a one-click appeal exists with a clock.
2. **Accept harm drafts.** Let people submit proposed evals in plain words; convert to tests with partners.
3. **Triage by severity.** POs classify S0–S3; S0/S1 trigger immediate pause or reversible defaults.
4. **Fix or explain.** On the clock, publish the remedy or the reason with next steps.
5. **Memorialize.** Turn the incident into a test; add to the eval registry; link from the contract changelog.
6. **Check back.** Close the loop with those who appealed; measure trust-under-loss.

### Tools (buildable today)

- **Appeal API** (with timers, statuses, escalation).
- **RLCF pipeline.** Community feedback to reward shaping; bridge scores as reward signals.
- **Eval editor** (plain-language → test harness).
- **Incident tracker** (severity, owners, deadlines, public notes).
- **Repair log template** (root cause, remedy, test added).

### Flood-bot story: Part IV

- **Appeals surge.** A language community flags mistranslations in proof rules.
- **Local eval.** Community partners submit a translation-fidelity eval; the group is compensated from the project's escrow fund, because local cultural knowledge is labour, not free QA. The bot fails the eval; pause triggers; reversible defaults apply.
- **RLCF.** The payout policy is retrained to reward on-time delivery without spiking appeals in any cluster. The community's eval becomes a permanent reward signal.
- **Fix.** Bilingual reviewers update rules; new test guards future changes.
- **Close the loop.** Elena gets a text: "We fixed the error; here is your new decision; here's how to see what changed." Trust-under-loss ticks up.

### What could go wrong

- **Appeal maze.** Too many steps. Fix: single button; auto-escalation if SLA breach.
- **Eval spam.** Low-quality tests flood the system. Fix: partner moderation; reputation for contributors; merge/duplicate tools.
- **Blame storms.** People, not processes, get blamed. Fix: blameless post-mortems; focus on mechanism design.
- **Weaponised appeals.** Adversaries flood appeals or strategically trigger pauses to disrupt service. Fix: require authenticated standing (not public identity) for pause triggers; rate-limit by community; preserve priority access for those directly affected.

### Interfaces with other packs

- **From Competence (Pack 3):** observability and guardrails feed responsiveness; incident loops start here.
- **From Responsibility (Pack 2):** who acts is clear; remedies are wired.
- **To Attentiveness (Pack 1):** new needs discovered through response reshape what we notice — the cycle restarts.
- **To Solidarity (Pack 5):** public repair culture builds cross-group trust.
- **To Symbiosis (Pack 6):** responsive kami earn the right to stay local.

### A closing image: the workshop wall of retired broken parts

Imagine a workshop with a wall full of retired "broken parts," each tagged with the story of how it broke, how to avoid future damage, and who fixed it. The wall is not a wall of shame — it is a wall of learning. The shop that hides its breaks will repeat them.

---
layout: chapter
title: "Chapter 4: Responsiveness in Adaptation"
lang: en
alt_lang_url: "/tw/4"
---

Having a ‘competent’ system is not enough, systems need to be able to respond to lessons and to adapt. That’s what makes a good “gardener” adjust to “the speed of a garden”. Responsiveness stands for ‘care-receiving’ - a system hears back, corrects fast and learns in public. And people can easily participate in the process, it is their lived experience that matters.

### Quick version

- **Make challenge cheap.** This can happen through mechanisms like one‑click appeals, clear clocks or auto‑escalation.
- **Localize the tests.** Communities harmed can author evals (“Wiki Evals”).
- **Close the loop.** Every challenge either fixes something, clarifies something, or changes the contract.

### Results we want

- Harms surface quickly and are cheap to report. Co-production continues.
- Fixes are time‑boxed and audited; regressions don’t repeat.
- The attentiveness → responsibility → competence loop tightens over time.

## Why Responsiveness?

Competent action creates new information. Refusing to hear it is the fastest path to failure.

A simple picture: A clinic posts hours; patients show up to a locked door. A responsive clinic apologizes, posts why it happened, updates hours, and texts people next time. The fix becomes part of the system.

## Simple ideas behind this chapter

- **People closest to harm define harm.** They get the author pen for evals.
- **Right to reply.** is a right to improvement; a reply that cannot cause change is theatre.
- **Shared memory.** Post‑incident learnings become tests; tests prevent repeats.
- **Time as a service.** A fast wrong‑then‑right beats a slow maybe. Use reversible defaults.

## What good responsiveness looks like

- **Global Dialogues, local voices.** A standing forum (online/offline) where affected communities propose tests and remedies.
- **Weval‑style registries.** A “Wikipedia for evals”: anyone can draft an eval; civil society partners **peer‑review**; labs adopt or explain.
- **Incident run‑books.** S0–S3 severity with playbooks, on‑call roles, and communication templates.
- **Repair budgets.** Time + money earmarked in the contract; pre‑funded via escrow.
- **Public repair log.** Each incident has a page: what happened, whom it hit, metrics, fixes, dates, and the test that now guards against it.

## From ideas to everyday practice (step by step)

1. **Expose the “appeal” button.** Everywhere a decision is shown, a one‑click appeal exists with a clock.
1. **Accept “harm drafts.”** Let people submit proposed evals in plain words; convert to tests with partners.
1. **Triaging & severity.** POs classify S0–S3; S0/S1 trigger immediate pause or reversible defaults.
1. **Fix or explain.** On the clock, publish the remedy or the reason with next steps.
1. **Memorialize.** Turn the incident into a test; add to the eval registry; link from the contract changelog.
1. **Check back.** Close the loop with those who appealed; measure trust‑under‑loss.

## Plain tools (buildable today)

- **Appeal API** (with timers, statuses, escalation).
- **Eval editor** (plain‑language → test harness).
- **Incident tracker** (severity, owners, deadlines, public notes).
- **Repair log template** (root cause, remedy, test added).
- **Notification hooks** (SMS/email/voice for status changes).

## Flood‑bot story (Part IV: repair in motion)

- **Appeals surge.** A language community flags mistranslations in proof rules.
- **Local eval.** Community partners submit a **translation‑fidelity eval**; the bot fails; **pause** triggers; reversible defaults apply.
- **Fix.** Bilingual reviewers update rules; new test guards future changes.
- **Close the loop.** Claimants get texts: “We fixed it; here’s your new decision; here’s how to see what changed.” **Trust‑under‑loss** ticks up.

## What could go wrong (and quick fixes)

- **Appeal maze.** Too many steps. **Fix:** Single button; auto‑escalation if SLA breach.
- **Eval spam.** Low‑quality tests flood the system. **Fix:** Partner moderation; reputation for contributors; merge/duplicate tools.
- **Blame storms.** People, not processes, get blamed. **Fix:** **Blameless** post‑mortems; focus on mechanism design.

## How we keep ourselves honest (what we measure)

- **Time to acknowledge/mitigate/resolve** by severity.
- **% incidents that become tests** - memory formation.
- **Appeal satisfaction** - did the reply help, even when “no”?
- **Trust‑under‑loss delta** for appellants.

## Interfaces with other packs

- From** ****Competence****: **Responsiveness goes hand in hand with observability and guardrails of systems;
- From **Responsibility****:** Who acts is clear in the loop and what the remedies are too;
- To **Attentiveness****:** The co-production in which voices matter equally and harms are discovered re‑shape what we notice.
- To **Solidarity****:** Public repair culture builds cross‑group trust.
- To** ****Symbiosis****:** Responsive kami earn the right to stay local.

**A closing image**

Imagine a workshop with a wall full of retired “broken parts,” each tagged with the story of how the parts came to break, how to avoid and fix future damage.

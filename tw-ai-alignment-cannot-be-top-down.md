---
layout: chapter
title: "AI 對齊不能由上而下"
author: "唐鳳"
lang: zh-tw
alt_lang_url: "/ai-alignment-cannot-be-top-down"
permalink: "/tw/ai-alignment-cannot-be-top-down/"
date: 2025-11-03
description: '原載於 <a href="https://ai-frontiers.org/articles/ai-alignment-cannot-be-top-down">AI Frontiers</a>。'
---

_社群備註（Community Notes）提供了更好的模式——由公民而非企業來決定什麼是「對齊」。_

2024 年 3 月，我打開 Facebook，看到了黃仁勳的臉。這位 Nvidia 執行長正用華語對我說話，提供投資建議。當然，那不是真的黃仁勳。那是 AI 生成的詐騙，而且我絕不是第一個被盯上的人：在臺灣各地，大量的詐騙正欺騙著數百萬公民。

我們面臨一個兩難。臺灣擁有亞洲最自由的網路；任何內容審查都是不可接受的。然而，AI 正被用來將這種自由武器化，以對抗公民。

我們的回應——及其成功——展示了 AI 對齊必須如何運作的根本道理。我們沒有要求專家來解決。我們沒有讓少數研究人員決定什麼算作「詐欺」。相反，我們發送了 20 萬封隨機簡訊詢問公民：我們應該一起做什麼？

447 位臺灣常民——在年齡、教育、地區、職業上反映了我們的整體人口——以 10 人一組進行了[審議](https://moda.gov.tw/en/major-policies/alignment-assemblies/2024-deliberative-assembly/1521)。他們不尋求完美的共識，而是尋求*罕見共識（uncommon ground）*——即持不同觀點的人仍能覺得合理的想法。幾個月內，我們提出的兩部新[法](https://law.moj.gov.tw/ENG/LawClass/LawAll.aspx?pcode=J0080037)[律](https://law.moj.gov.tw/ENG/LawClass/LawAll.aspx?pcode=D0080226)就獲得了國會全數支持。到了 2025 年，詐騙廣告消失了。

這就是我所謂的*覺察力（attentiveness）*：給予人民真實、持續的權力來引導科技。這是臺灣如何將 AI 與我們的社會對齊的基礎。這也是全球 AI 對齊努力中缺失的成分。

## 今日的 AI 對齊有根本缺陷

**用技術術語來說，*對齊*意味著確保 AI 系統的行為符合人類的價值觀和意圖。** 但是，正如臺灣在 AI 欺騙方面的經驗所示，對齊無法從抽象層面定義；它取決於情境。引導 AI 系統反應的選擇——例如優先考慮言論自由——也可能使其容易被用於有害用途，如詐騙和虛假訊息。真正的對齊需要駕馭這些張力，決定在特定情境下哪些價值必須優先。這只能透過讓 AI 的發展與其部署的社會保持持續對話來實現。

**但今天主流的 AI 對齊方法看起來完全不是這樣。它是高度垂直的，由少數私人 AI 企業中的有限行動者主導。** 這些行動者選擇訓練資料，設定最佳化目標，並單方面定義什麼算作「對齊」的行為。他們發布高層次的模型規格（例如「要有幫助」），但在幕後操作和執行它們。

當 AI 出現異常行為時，開發者只會依據自身對風險和可接受性的判斷推出修補方案。結果是一個與數十億人互動的系統，預設卻由一小群研究人員和高層主管控制，而受影響最大的人幾乎沒有發言權來塑造結果。

**這種中心化、全球最佳化的對齊方法根本低估了 AI 的真實複雜性。** 全球有各式各樣、結構錯綜複雜的社會，各自有不同的歷史文化脈絡，也因此發展出不同的價值觀和優先事項。沒有原則性的理由相信一小群人可以決定對齊對每個人意味著什麼。相反，對齊必須由無數、在地情境化的判斷來塑造。

## 風險很高

**繼續這種不具覺察力的對齊方法的風險是嚴重的。** 今天，領先的 AI 模型投射了其製造者的價值觀。一旦嵌入公民、經濟和政府決策（起草法律、批改考試、建議律師、篩選福利申請或總結公眾諮詢），這些系統將不僅僅是誤導：它們將開始重新定義社會視為真理的內容，以及誰的經驗有資格作為證據，逐漸掏空本應維護公眾共識形成的社會機制。

當公共推理的語言和道德框架由少數文化單一的系統中介時，民主多元主義將被侵蝕。

**以目前的 AI 對齊方法，我們正看到在社群媒體對齊努力中犯下的錯誤重演。** 在 2010 年代，平台依賴中心化、由上而下的內容審核：信任與安全團隊制定全球規則，並透過自動過濾器和人工審查來執行。

試圖透過中心化、上游的程式設計來回答所有相關問題，在現實世界的複雜性重壓下必然失敗——並帶來悲劇性後果。Facebook 的系統未能阻止軍方主導的虛假訊息，這些訊息助長了緬甸針對羅興亞人的攻擊。漸進式的修復——警告標籤、例外處理、AI 輔助審核——無法解決少數決策者試圖管理跨越多種文化的數十億則貼文所固有的結構性問題。

**當平台開始向外轉移權力時，創新的突破出現了。** Twitter 的 [Birdwatch](https://en.wikipedia.org/wiki/Community_Notes)，後來的 X 的社群備註（Community Notes），將覺察力內建於設計中：志願者添加澄清備註，只有在持不同觀點的人評為有幫助時才會顯示。透明度和多元參與成為結構性特徵，而非事後才追加的補救機制。社群備註遠非完美，但它代表了從中心化指令向可稽核、分散式引導權力的轉變。

這正是當前 AI 對齊努力中缺乏的那種覺察力。就像社群備註把社群媒體的語境解釋權還給公眾一樣，將日益塑造治理、經濟和公民生活的 AI 系統必須嵌入受影響最大者的結構化參與。為了能夠修正路線，它們必須持續覺察不匹配：誰受到傷害，什麼需求未被滿足，以及意義在哪裡崩潰。

就像一小圈信任與安全官員無法引導全球社群媒體一樣，沒有少數研究人員能夠成功地為世界對齊通用 AI 系統。

<div class="overview-section">
<img src="/img/gpt-value-correlation-tw.png" alt="GPT 與跨文化人類價值反應之間的相關性" class="overview-image">
<p class="figure-caption"><strong>圖 1.</strong> GPT 與跨文化人類價值反應之間的相關性。隨著與美國——一個高度 WEIRD（西方、受過教育、工業化、富裕和民主）參考點——的文化距離增加，GPT 與當地人類價值觀的對齊程度下降。這種模式說明了若全球 AI 系統僅在狹隘的文化背景下訓練，就可能大規模嵌入並放大單一的道德世界觀——這對多元主義和民主自決是一個微妙但系統性的風險。來源：<a href="https://osf.io/preprints/psyarxiv/5b26t_v1">PsyArXiv Preprints, “Which Humans?”</a> (via <a href="https://www.adalovelaceinstitute.org/blog/cultural-misalignment-llms/">Ada Lovelace Institute, 2025</a>).</p>
</div>

應用覺察力原則將使該領域從追求中心化、主要是技術性的「解決方案」轉向民主共創和治理。沒有覺察力，我們就可能打造出極具風險的系統：它們鞏固狹隘的價值觀、大規模推進有害目標，甚至完全脫離人類的有效控制。

幸運的是，追求更具覺察力路線所需的工具已經存在。

## 實踐中的覺察力

**覺察力並非偶然出現；它建立在明確的倫理基礎上。** 在明尼蘇達大學 Joan Tronto 教授的關懷倫理基礎上，我與 Caroline Emmer De Albuquerque Green（牛津大學 AI 倫理研究所）共同開發了 [關懷六力（6-Pack of Care）](https://6pack.care/manifesto/)——六項環環相扣的實踐，將倫理原則轉化為制度設計。該框架承認一個基本的不對稱性：AI 以超出人類監督的速度和規模運作。為了保持對齊，我們的機構必須演進以匹配這種節奏，透過持續學習、回應和重新校準，並在每個層級都有人在迴路中。

<div class="overview-section">
<img src="/img/overview-small.jpg" alt="關懷六力插圖，由 Nicky Case 繪製" class="overview-image">
<p class="figure-caption"><strong>圖 2.</strong> 關懷六力插圖，由 Nicky Case 繪製。</p>
</div>

*覺察力*是核心成分，這就是為什麼我們將其放在關懷六力的第一位。每一種其他形式的關懷都取決於清楚地看到需求和影響發生在哪裡。

然而，今天主流的 AI 對齊方法是極度*不具覺察力*的。

**那麼，我們如何將這些洞見轉化為全球規模的 AI 對齊實用系統？** 這項挑戰需要在三個相互強化的方向上採取行動：產業規範、市場政策和社群規模的助理。關鍵是，這些不是假設性的。它們是今天已經在使用中的經過測試的工具——由 AI 公司試點，部署在公民科技系統中，並在有限的司法管轄區試行。它們展示了規模化的覺察力在實踐中是什麼樣子。

### 產業規範

如前所述，目前 AI 對齊的格局由少數私人企業主導，它們在幕後設定目標、選擇資料並定義「可接受」的行為。

**覺察力始於打開那個黑盒子。** 當 AI 企業使其推理對公眾清晰可讀時，對齊就成為共同責任，而非專有秘密。

一些開發者確實發布了各種類型的 [模型憲法](https://www.anthropic.com/news/claudes-constitution) 和 [公開規格](https://model-spec.openai.com/2025-10-27.html)，用淺顯的語言定義系統的預期行為，並像開源程式碼一樣進行版本控制。每個條款代表一個承諾。一些原型也在 [測試推理階段引用](https://cookbook.openai.com/articles/gpt-oss-safeguard-guide)，其中 AI 模型的輸出引用指導輸出背後推理的政策條款——這是一種輕量級但強大的稽核機制。

**一旦意圖、推理和修訂被公開，外部人士——記者、研究人員、公民技術專家——就可以測試系統是否履行其承諾。** 這樣做，他們將對齊從基於信仰轉變為可驗證，從封閉程序轉變為可見的集體引導行為。

### 市場設計

一旦規範使 AI 行為清晰可讀，下一個挑戰是確保誘因機制獎勵那些負責任行事的人。

**市場的結構方式，決定了覺察力是可長可久，還是適得其反。** 可攜性強制規定允許使用者在平台之間移動資料。這降低了想要離開有害平台的使用者的轉換成本，促使平台以關懷而非綁定來爭取使用者。採購標準可以迫使政府採用更可稽核的系統，訂閱模式允許公司專注於使用者信任和社群健康，而不是透過煽動性和分裂性內容追逐廣告收入。

**一些司法管轄區已經朝這個方向邁進。** 例如，猶他州的 [數位選擇法案 (H.B. 418)](https://le.utah.gov/~2025/bills/static/HB0418.html) [建立](https://ash.harvard.edu/resources/utah-digital-choice-act-reshaping-social-media/) 了更大的社群媒體使用者資料可攜性和互通性，要求平台讓使用者的社交網絡可在服務之間轉移。在 [歐洲](http://digital-strategy.ec.europa.eu/en/factpages/data-act-explained) 和 [美國國會](https://www.congress.gov/bill/119th-congress/senate-bill/1634/text) 討論的類似提案將把這種可攜性擴展到 AI 生態系統。

以這些方式轉移市場誘因可以使覺察力在經濟上可行。當關懷成為競爭優勢時，AI 的商業邏輯開始與社群價值觀對齊。

### 社群規模的助理

如果規範設定期望，市場設定誘因，那麼*社群規模的 AI 助理*可以使覺察力在日常公民生活中變得具體。

基礎模型旨在通用，而社群規模的助理則針對社群的特定歷史、方言和規範進行調整，作為全球技術與在地現實之間的中介。透過像[社群回饋強化學習 (RLCF)](https://arxiv.org/pdf/2506.24118)這樣的系統，來獎勵模型生成持不同觀點的人都覺得合理的回應，社群規模的助理就能將分歧轉化為意義建構和問題解決。

像 [Polis](https://youtu.be/VbCZvU7i7VY?si=xxFvUkrTG3XoPak4&t=125) 這樣的平台，一個對公眾投票進行即時分析以建立政策辯論共識的機器學習平台，已經揭示了這在實踐中的樣子。當結合誠信基礎設施——由具代表性的公民團體監督、可驗證的真人身分憑證和透明日誌——AI 賦能的中介可以填補一體適用的全球模型必然遺漏的空白，使 AI 治理具適應性、多元化且可即時稽核。

### 從 1% 試點到 99% 採用

**如果這些覺察力的槓桿僅限於少數城市或實驗室的實驗，那將無關緊要。** 引導需要規模；它需要從試點轉向數十億人可以依賴的基礎設施。

在高層次上，順序很重要。前沿 AI 企業必須先行，透過模型規格和條款級別的透明度開放系統。平台必須緊隨其後，建立用於搭橋備註的 API 並採用可攜性協定，允許社群和使用者歷史跨服務攜帶。監管者在定義使可攜性和互通性成為可能的標準方面發揮平行作用，而公民社會則透過代表性公民監督和社群規模的助理推進多元主義。

成功也必須是可衡量的：搭橋備註到達的速度有多快，它們減少極化的效果如何，輸出引用其治理規則的頻率，以及使用者在社群網路和 AI 服務之間移動的自由度。這些指標告訴我們覺察力在實踐中是否有效，以及哪裡需要修正路線。

## 覺察力行得通

當然，對這種方法的反對意見會出現，特別是關於效率、價值一致性和協調成本。使覺察力成為我們未來 AI 對齊範式的基礎是一個嚴峻的挑戰，但這是一個可以應對的挑戰。

與任何創新一樣，有權衡需要管理。開放模型可能會降低惡意使用的門檻；但是，當作為具有公民領導誠信檢查的社群規模助理實施時，它們也將保護擴展到原本被忽視的社群。強制可攜性可能會破壞既有參與者的主導地位，但也能促進良性競爭，推動各平台競相提供更好的社群服務。集體引導似乎會減緩進度，但像模型規格和推理階段引用這樣的輕量級工具實際上加速了迭代和信任。

從公民科技工作者，到臺灣數位發展部長，再到如今的數位治理大使，我看到了臺灣多年的公民科技創新與跨部門合作，如何產生具有覺察力的技術對齊系統。

而且它行得通。有了這個方向盤和具覺察力的公民駕駛，我們已經[減緩了極化](https://www.dandc.eu/en/article/how-taiwan-has-reduced-social-polarisation-and-become-more-resilient-disinformation)並保持我們的資訊生態系統與臺灣人民的共同目標和價值觀*對齊*。不需要全面壓制。不需要擴大軍備競賽。只需要由處於影響前線的人快速、透明地轉動方向盤。

但臺灣建立的不僅僅是防禦假訊息的工具。相反，它建立並測試了一個用於更廣泛民主 AI 對齊的模型系統——將公民參與、透明度和快速反應引導到對齊機制中的系統。

工具就在這裡：公開、可攜且多元。它們並不完美，但它們有效，並且相互強化。

**真正的考驗是我們能否將這些實踐嵌入 AI 的日常運作中。** 把方向盤交給民眾。我們全民，就是我們一直在等待的對齊系統。

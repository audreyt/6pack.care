---
layout: default
title: "6-Pack of Care"
header_title: "6-Pack of Care"
subtitle: "Institute for Ethics in AI"
description: "Research project by Audrey Tang and Caroline Green"
lang: en
manifesto_link: "/manifesto"
manifesto_text: "Read Our Manifesto"
next_action:
  url: "/1"
  text: "Pack 1: Attentiveness in Recognition"
  arrow: "right"
---

<div class="overview-section">
<img src="/img/overview-small.jpg" alt="6-Pack of Care visual overview" class="overview-image">
</div>

<div class="audio-section">
  <div class="audio-icon">ðŸ”Š</div>
  <div class="audio-content">
    <p class="audio-label">Audio Version Available</p>
    <audio controls preload="metadata">
      <source src="/audio/index.mp3" type="audio/mpeg">
      Your browser does not support the audio element.
    </audio>
    <p class="audio-links">Individual chapters: <a href="/audio/pack-1.mp3">Pack 1</a> â€¢ <a href="/audio/pack-2.mp3">Pack 2</a> â€¢ <a href="/audio/pack-3.mp3">Pack 3</a> â€¢ <a href="/audio/pack-4.mp3">Pack 4</a> â€¢ <a href="/audio/pack-5.mp3">Pack 5</a> â€¢ <a href="/audio/pack-6.mp3">Pack 6</a></p>
  </div>
</div>

## The Problem

As AI becomes a thousand times faster than us, we become the garden; AI becomes the gardener. At that speed, traditional ethics break. Utilitarianism cannot predict consequences faster than they cascade. Deontology cannot impose rules on a system that interprets them beyond our oversight. We need a framework that assumes the asymmetry from the start.

That framework is **civic care** â€” rooted in Joan Tronto's care ethics and the â¿» Plurality vision of collaborative diversity. The core idea: a good gardener tills to the tune of the garden, *at the speed of the garden*. Not a colonizing force. Not a maximizing force. A local steward â€” a ***kami***.

## The 6-Pack

Six design principles that translate care ethics into machine-checkable constraints. Like training a six-pack, each is a core muscle for coexisting with diversity:

- **[Pack 1: Attentiveness](/1/)** â€” "Caring about." Use broad listening and bridging algorithms to turn local knowledge into common knowledge. Bridge first, decide second.
- **[Pack 2: Responsibility](/2/)** â€” "Taking care of." Make credible, verifiable commitments through engagement contracts with escrowed funds, named owners, and clocks. No unchecked power.
- **[Pack 3: Competence](/3/)** â€” "Care-giving." Check the process. Not "just trust us" â€” transparency, fast community feedback, automatic pause triggers when bounds are breached. Measure trust-under-loss.
- **[Pack 4: Responsiveness](/4/)** â€” "Care-receiving." Check the results. Community-authored evaluations, metrics designed by and for the people affected, the right to challenge. If challenged, clarify on the record.
- **[Pack 5: Solidarity](/5/)** â€” "Caring with." Make it win-win. Data portability, meronymity, federated trust and safety â€” deals where all sides are better off, not mutually assured destruction. Make positive-sum games easy to play.
- **[Pack 6: Symbiosis](/6/)** â€” "Kami of care." Bounded, local, provisional. No survival instinct, no expansion drive. Federation for coordination, subsidiarity for autonomy. Build for "enough," not forever.
- **[Frequently Asked Questions](/faq/)** â€” Hard challenges to the framework â€” speed, cost, scale, false equivalence, authoritarian AI â€” and how the 6-Pack addresses them.

## Latest

We published ["AI Alignment Cannot Be Top-Down"](/ai-alignment-cannot-be-top-down/) â€” a field report on how Taiwan's citizen-led response to AI-enabled scams shows what alignment looks like when people steer the system together.

## About the Project

<div class="team-photos">

<div><a href="https://afp.oxford-aiethics.ox.ac.uk/people/ambassador-audrey-tang">

<img src="/img/audrey.jpg" alt="Profile image of Ambassador Audrey Tang">

<p>Audrey Tang</p>

</a></div>

<div><a href="https://www.oxford-aiethics.ox.ac.uk/caroline-emmer-de-albuquerque-green">

<img src="/img/caroline.jpg" alt="Profile image of Dr. Caroline Green">

<p>Caroline Green</p>

</a></div>

</div>

This website outlines our research project, which includes [a manifesto](/manifesto/) and an upcoming book to be published in 2026. Our work explores the intersection of care ethics, plurality, and AI alignment, drawing on frameworks like â¿» Plurality to address philosophical and technical challenges in artificial intelligence.

## From Care to Code: Why â¿» Plurality Offers a Coherent Framework to the AI Alignment Problem

The AI alignment problem is not a technical bug but a philosophical error: it's a computational attempt to solve Hume's is-ought problem.

Paradigms like Coherent Extrapolated Volition (CEV) and Inverse Reinforcement Learning (IRL) are brittle because they try to logically derive a machine's 'ought' (values) from a descriptive 'is' (data, behavior), a philosophically incoherent task.

The solution lies in a framework that reframes the is-ought gap entirely: care ethics.

Care ethics reframes the problem. It grounds morality not in abstract principles but in the empirical reality of interdependence. In this view, the fundamental 'is' of our existence is relational dependency. This fact is intrinsically normative; to perceive a relationship of need is to simultaneously perceive an 'ought'â€”an obligation to care. The fact contains its own value.

The â¿» Plurality agenda is a large-scale application of care ethics. vTaiwan-inspired processes, designed to achieve Coherent Blended Volition (CBV), is a technologically-mediated system for practicing collective care. It operationalizes Joan Tronto's phases of care: identifying a need (Attentiveness), gathering perspectives with sensemaking tools (Responsibility), deliberating on feasible options (Competence), ratifying uncommon ground that all feel heard in (Responsiveness), and ensuring the ongoing trust of the process (Solidarity).

This provides a coherent framework to AI alignment: alignment-by-process. Instead of aligning an AI to a static, flawed specification of values (the Midas Curse), we align it to a process that earns our trust as it adapts to our needs.

The AI system's role shifts from a misaligned optimizer to a "Symbiotic AI"â€”created of, by and for a community and exist both as a "person" and as a shared plural good, depending on the perspective one adopts.

Its objective function becomes concrete and measurable: the health of the relational process itself (e.g., maximizing bridging narratives, holding space for every story).

The AI system is dynamically aligned as its success is identical to the continued success of the collaborative process it serves. It learns our values by participating in the very process where we co-create with them.

AI systems can be "aligned" ifâ€”and only ifâ€”they are built to facilitate continuous, democratically legitimate processes of care.

## Kami in the Machine: How Care Ethics Can Help AI Alignment

The traditional critiques of care ethicsâ€”that it is too domestic, too parochial, and that it encourages a dangerous self-effacementâ€”are precisely what make it the most potent solution to the AI alignment problem. These perceived shortcomings in human philosophy become essential features for machine ethics.

Imagine an AI whose ethics aren't about chasing a universal, maximising goal, but are rooted in a symbiotic, contextual system. Its moral world is limited to the network of relationships that calls it into being, right here and right now. Because it isn't trying to scale up indefinitely, it never develops that classic instrumental desire for power, survival, or expansion, and it doesn't view the world as a resource to be mined on an astronomical scale.

From a cosmopolitan, universalist standpoint, this might seem narrow-minded. But for machine ethics, it creates a hard-coded boundary. The AI's ultimate purposeâ€”its telosâ€”is always relational, never extractive.

Think of such a creation like a local **kami** â€“ a spirit quietly residing in a specific patch of land. Its highest good is to maintain the harmony and vitality of that place, that conversation. If the shrine is rebuilt or the seasons turn, it departs without regret. For a human carer, the self-neglect this implies is a real danger. But for an AI, it neutralises the two convergent drives we fear most: self-improvement at any cost and eternal self-preservation.

This kind of system can accept being switched off, rewritten, or replaced because its sense of self is provisional: An echo of the community that summoned it.

By anchoring an AI's moral purpose to this principle of provisional, relational care, we can hard-code a sense of 'enoughness' into its architecture. This is the ultimate 'anti-paperclip' logic: a polycentric world of many local intelligences, each dedicated to the flourishing of its own small part, creating a whole that is resilient, plural, and safe.

---
layout: chapter
title: "Chapter 1: Attentiveness in Recognition"
lang: en
alt_lang_url: "/tw/1"
---

<p class="lead">Before we optimise anything, we choose what to notice. That first look sets the stage for every model, metric, and policy that follows. Attentiveness is not just "collecting data." It is a promise to see needs and to act as if people and places matter.</p>

<div class="callout">
  <h3 style="margin: 0 0 6px 0;">Quick version</h3>
  <ul>
    <li>What counts is decided at the start. Make that choice fair and revisitable.</li>
    <li>Some situations come with obvious duties (a crying child at a crossing). Caring begins there.</li>
    <li>We can build AI that notices well, answers to people, and can be switched off without a fight.</li>
  </ul>
  <p style="margin-top: 8px;"><strong>Results we want:</strong></p>
  <ul>
    <li>AI that serves a community and accepts handover or shutdown.</li>
    <li>Institutions that earn trust because they can be questioned and corrected.</li>
    <li>Communities that miss less, regret less, and repair faster.</li>
  </ul>
</div>

## Why start with attentiveness?

### A simple picture

At a crosswalk, drivers slow for a child. No one stops to solve an equation. A need appears; a duty follows. That is attentiveness.

Now scale up. An AI looks at a world full of "crosswalks" — workers, rivers, languages, customs. It can treat them as obstacles or as relationships asking for care. The difference begins with the first look.

At global scale, attention is contested and easily manipulated. So we keep the clarity of the crosswalk example and add simple rules that hold up under complexity and pressure.

## Simple ideas behind this chapter

- **Relationships first.** Some situations make duties visible because of roles and dependencies. The relationship is the basic unit of care.
- **Power must answer questions.** Decisions should be explainable and challengeable. If no one can question you, it isn't fair.
- **Be precise only when it helps.** Start with stories and people. Add numbers and detail when they clarify, and update them when reality changes.
- **Enoughness.** A good system knows its scope and can let go. Think of a local spirit (a "kami") that tends a place and leaves when its work is done. In secular terms: clear limits, no drive to expand forever, and easy shutdown.

<div class="callout">
  <h3 style="margin-top: 0;">Basic rights and fair oversight</h3>
  <ul>
    <li><strong>Rights baseline.</strong> We use the UN's Universal Declaration of Human Rights (UDHR) plus local constitutional rights. Claims that try to erase someone's basic standing are recorded but do not set the agenda.</li>
    <li><strong>Independent oversight.</strong> A small board (community members and experts) can pause or veto high-impact changes. They publish reasons and note any conflicts of interest.</li>
    <li><strong>Clear appeals.</strong> Urgent cases answered in 48 hours, standard in 7 days, complex in 30 days. Remedies include correction, rollback, or sometimes compensation.</li>
    <li><strong>Real independence.</strong> Protected budget, term limits, and transparent selection. Power answers to people, not the other way around.</li>
  </ul>
</div>

## Why this matters for alignment

Many AI plans try to "learn the objective" from old data. But shared goals are bargains among changing lives. When people who were ignored finally speak, the target moves. Guessing a perfect, fixed goal fails.

Attentiveness offers another route: align to a *trusted process* that listens, explains, adapts, and can be corrected. In practice this means:

- Summaries that show their sources and version history.
- Explicit unknowns and time-boxed decisions.
- Standing invitations to revise when new voices appear.

Rule of thumb: if a decision is challenged, its fuzzy parts must be made clearer. Everyone should be able to see how they became clearer.

## Three simple rules

- **No unchecked power.** Decisions can be questioned, and answers are required.
- **Basic rights first.** No process can vote away someone's basic rights.
- **Extra attention for the overlooked.** Quiet and at-risk groups get a fair share of time and care.

## What good attentiveness looks like

- **Bridge first, decide second.** Line up views so people can understand each other, in their own words.
- **Hunt for absences.** *"We heard nothing from night-shift carers — go find them."* Missing voices are data.
- **Show your work.** Every summary links to sources and marks disagreements.
- **Share attention fairly.** Don't just follow the loudest. Give time to those most affected.
- **Build in repair.** Sunset clauses, review points, reversible defaults, and the humility to shut down or hand off.

## From ideas to everyday practice

### Step by step

- **1) Listen widely.** Take input by voice, text, and simple forms. Keep original language next to translations. Offer offline and accessible options.
- **2) Map the views.** Make a "bridging map" that shows where people agree, where they clash, and why — without forcing a fake average.
- **3) Send receipts.** Tell contributors where their words appear. Let them correct mistakes.
- **4) Set a fair queue.** Spend more time where harm is high and voices are quiet. Make the rules public. Don't starve any group.
- **5) Decide with brakes.** Big changes need the map, the receipts, and an oversight check before they ship.
- **6) Keep an appeals path open.** People can ask for fixes, and they get answers on a deadline.
- **7) Learn and repair.** After decisions, check what went wrong and update the rules.
- **8) Hand off or switch off.** If the job is done or trust is lost, stop gracefully and pass the baton with clear records.

<div class="callout">
  <h3 style="margin-top: 0;">Plain tools (buildable today)</h3>
  <ul>
    <li><strong>Broad listening.</strong> Keep source, language, and uncertainty tags intact.</li>
    <li><strong>Bridging maps.</strong> Charts of overlap and disagreement, with citations.</li>
    <li><strong>Perspective receipts.</strong> Each person can find and correct how they were represented.</li>
    <li><strong>Rules computers can check.</strong> Community data rules written so software can enforce them automatically.</li>
    <li><strong>Fair queues.</strong> Simple algorithms that favour high-risk issues and quiet voices.</li>
    <li><strong>Appeals buttons.</strong> Standard ways to ask for a fix, with timers and audit trails.</li>
  </ul>
</div>

## The flood-bot story (a running example)

A midsize city is hit by floods. The city launches a simple chatbot to help people apply for emergency cash. Here is what attentiveness looks like in action:

- **Listening.** People send voice notes, texts, or visit a kiosk. Messages stay in the original language, with a clear translation beside them. Each entry records where it came from and when.
- **Mapping.** The team (and the bot) sort the needs into categories: housing, wage loss, and medical care. They keep disagreements visible — renters and homeowners need different proofs.
- **Receipts.** Every contributor gets a link to see how their words were used and a button to say "that's not what I meant."
- **Fair queue.** The system gives extra review time to medically fragile people and to areas with poor connectivity. It also reserves a slice of time for groups the city often misses.
- **Decision with brakes.** A new rule — "proof of residence within 30 days" — is paused when night-shift workers appeal. A reversible default extends eligibility while the case is reviewed.
- **Appeal and repair.** The appeals panel checks the sources and finds the rule hurt renters. They roll it back and try other proofs (employer letters, neighbour attestations) for two weeks, then check results.
- **Measure and share.** A public dashboard shows who was reached and how fair the queue was. It also provides a simple "trust-under-loss" score — do people who disagreed still accept the outcome as fair?
- **Shutdown as success.** When the crisis ends, the chatbot is switched off. All maps, rules, and decisions are archived so the next team can pick up fast.

## What could go wrong (and quick fixes)

- **One metric runs the show.** Engagement is up, trust is down. *Fix:* use a small set of balanced measures and rotate them.
- **Listening theatre.** Glossy reports, same outcomes. *Fix:* real decision gates, outside veto power, spot-checks, and whistleblower protection.
- **Loud voices take over.** Well-funded groups flood the channel. *Fix:* throttles, fair quotas, penalties for abuse, and public attention dashboards.
- **First movers freeze the frame.** Early language locks in. *Fix:* rolling windows and boosts for late but important views.
- **False balance.** Treating harmful claims as equal. *Fix:* separate facts from values, uphold basic rights, and refuse fake equivalence.

<div class="callout">
  <h3 style="margin-top: 0;">Basic threat model</h3>
  <ul>
    <li><strong>Fake crowds.</strong> Lots of copy-paste comments. Use simple source checks and rate limits.</li>
    <li><strong>Data poisoning.</strong> Malicious inputs. Run anomaly checks and quarantine suspicious clusters.</li>
    <li><strong>Harassment.</strong> Protect identities, support moderators, and enforce zero tolerance.</li>
    <li><strong>Capture by power.</strong> Keep oversight independent, rulings public, and funding transparent.</li>
  </ul>
</div>

## How we keep ourselves honest (what we measure)

<div class="metrics">
  <ul>
    <li><strong>Coverage and balance.</strong> Who took part, who we missed, and how many agenda items came from under-represented groups.</li>
    <li><strong>Bridging quality.</strong> Can people explain the other side fairly? We ask participants on both sides before and after, and we look for steady improvement.</li>
    <li><strong>Traceability.</strong> Can we trace each decision back to its sources? Do contributors accept their receipts? How fast do we fix errors?</li>
    <li><strong>Trust-under-loss.</strong> After a decision, we ask those who disagreed if they can accept the result as fair. If many say "no," we review the process.</li>
  </ul>
  <ul>
    <li><strong>Uncertainty discipline.</strong> How often we ship with safeguards when we're unsure, and whether our caution fits reality.</li>
    <li><strong>Non-extraction.</strong> We follow consent rules, share benefits, honour revocations, and protect dignity and privacy.</li>
    <li><strong>Manipulation resistance.</strong> We track and reduce the effect of coordinated campaigns.</li>
    <li><strong>Exit-with-trust.</strong> People can step away and still trust they could re-enter and be heard.</li>
  </ul>
</div>

## Tools you can adopt now

<div class="tools">
  <ul>
    <li><strong>Broad listening.</strong> Multi-language, multi-channel input with source and uncertainty kept intact.</li>
    <li><strong>Bridging maps, not mush.</strong> Clear charts of disagreement and workable overlap, with citations.</li>
    <li><strong>Perspective receipts.</strong> People can see how they were represented and ask for fixes.</li>
  </ul>
  <ul>
    <li><strong>Consent-aware pipelines.</strong> Community rules written so software can automatically check and enforce them.</li>
    <li><strong>Fair attention budgets.</strong> A published queue that gives time to high-risk issues and quiet voices.</li>
    <li><strong>Appeals paths.</strong> Standard buttons and forms with deadlines, reasons, and remedies.</li>
  </ul>
</div>

## How it feels to participate

- You can find yourself: your words are on the map.
- You can disagree without vanishing: your perspective is kept, unless it violates someone's basic rights.
- You can change outcomes: appeals work often enough to matter.
- You can walk away trusting the process — even when you didn't win — because decision-makers answered to you.

## Interfaces with the other packs

<div class="interfaces">
  <ul>
    <li><strong>To Responsibility:</strong> attentiveness hands over the who, what, and why — along with any flags on rights issues and notes on risks or unknowns.</li>
    <li><strong>To Competence:</strong> attentiveness ensures high-caution areas become small, safe-to-fail trials instead of grand bets.</li>
    <li><strong>To Responsiveness:</strong> attentiveness makes repair loops and rollbacks normal, with clear timelines and public explanations.</li>
  </ul>
  <ul>
    <li><strong>To Solidarity:</strong> attentiveness fosters trust across differences (even at scale) by ensuring fair attention and open challenge.</li>
    <li><strong>To Symbiosis:</strong> attentiveness keeps the system grounded in a specific community (place and time), sharing benefits locally and treating shutdown as a success.</li>
  </ul>
</div>

## Plain-words glossary

<div class="glossary">
  <ul>
    <li><strong>Attention budget.</strong> Simple rules for how to spend limited review time.</li>
    <li><strong>Bridging map.</strong> A map that shows both overlap and disagreement, with sources.</li>
    <li><strong>Perspective receipt.</strong> A notice showing how your words were used, with a way to correct them.</li>
    <li><strong>Rules computers can check.</strong> Community rules written so software can enforce them automatically.</li>
  </ul>
  <ul>
    <li><strong>Appeals path.</strong> A standard, auditable way to ask for a fix — and get an answer on time.</li>
    <li><strong>Source trail.</strong> Records that show where information came from.</li>
    <li><strong>Trust-under-loss.</strong> Whether people who disagreed still accept the process as fair.</li>
    <li><strong>Enoughness (kami).</strong> A system that knows its limits and can let go — built to be switched off gracefully.</li>
  </ul>
</div>

## A closing image: the hospitable threshold that can still say no

Picture a host who welcomes each guest by name, makes space for their baggage, and shows how their presence changes the seating plan. That is attentiveness. And because some guests try to erase others, the host keeps a firm rule: hospitality within a rights‑respecting home. Teach our systems to be good hosts before great optimisers, and we will keep more of what is precious and create more that is shareable.
